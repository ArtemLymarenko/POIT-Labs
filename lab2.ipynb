{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**1. Визначити сутність і виконати аналіз функцій парного блокуючого обміну.**\n",
    "\n",
    "Парний блокуючий обмін - це базова модель взаємодії двох процесів у MPI, коли один процес надсилає повідомлення, а інший його приймає, і обидві операції є блокуючими, тобто процес зупиняється, доки виклик не завершиться.\n",
    "\n",
    "У парному обміні беруть участь два процеси: відправник і одержувач.\n",
    "\n",
    "Відправник викликає функцію MPI_Send, передаючи адресу буфера, тип даних, кількість елементів, тег повідомлення та ранг процесу-одержувача.\n",
    "\n",
    "Одержувач викликає MPI_Recv з аналогічними параметрами, вказуючи, від кого він чекає повідомлення та який тип і тег має відповідати.\n",
    "\n",
    "Блокуюча відправка означає, що процес-відправник не продовжить виконання, поки MPI не буде повністю впевнена, що параметри буфера можна безпечно змінювати - або тому, що дані вже гарантовано передані, або тому, що вони скопійовані у внутрішній буфер MPI.\n",
    "\n",
    "Важливою частиною парного обміну є узгодженість параметрів: обидві сторони мають співпасти за типом даних, розміром та тегом. Це забезпечує коректність і запобігає неоднозначності.\n",
    "\n",
    "Блокуючі виклики забезпечують ряд важливих властивостей.\n",
    " * Кожен MPI_Recv отримає саме те повідомлення, якого він очікує.\n",
    " * Вони забезпечують локальну синхронізацію між процесами. Хоча MPI_Send не зобов’язаний чекати фактичного прийому, у більшості реалізацій прості повідомлення передаються одразу, і відправник блокується тільки до моменту копіювання в буфер MPI.\n",
    " * Парний блокуючий обмін може бути джерелом взаємних блокувань (deadlock), якщо обидва процеси викликають MPI_Send, не маючи активних прийомів. Наприклад, двостороння відправка без попереднього MPI_Recv може заблокувати програму, якщо реалізація MPI не має достатньо буфера для обох повідомлень.\n",
    " * Також, важливо розуміти його семантику: завершення відправки не означає, що повідомлення доставлене одержувачу. Завершення прийому означає, що дані гарантовано знаходяться у буфері одержувача."
   ],
   "id": "96a916e4d2a4ec9a"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-22T12:28:55.446487Z",
     "start_time": "2025-11-22T12:28:53.956041Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install mpi4py",
   "id": "a635142cc0110897",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpi4py in c:\\users\\artem\\pycharmprojects\\pythonproject\\distibutedcalculations\\jupyterproject\\.venv\\lib\\site-packages (4.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Тестова програма для перевірки роботи бібліотеки в середовищі Jupiter Notebook**",
   "id": "18ad33120ac6bc52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T12:28:55.472526Z",
     "start_time": "2025-11-22T12:28:55.467444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile test_mpi.py\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "print(f\"Rank: {comm.Get_rank()} | Size: {comm.Get_size()}\")"
   ],
   "id": "99e1330e475e7f46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_mpi.py\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T12:28:55.806907Z",
     "start_time": "2025-11-22T12:28:55.566799Z"
    }
   },
   "cell_type": "code",
   "source": "!mpiexec -n 4 python test_mpi.py",
   "id": "4f03dc2a3773d2e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1 | Size: 4\n",
      "Rank: 0 | Size: 4\n",
      "\n",
      "\n",
      "Rank: 2 | Size: 4\n",
      "Rank: 3 | Size: 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**2. Побудувати програму-шаблон для парного блокуючого обміну**",
   "id": "8a02bd4094765079"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T12:28:55.829430Z",
     "start_time": "2025-11-22T12:28:55.824657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile task2.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "# function to return whether a number of a process is odd or even\n",
    "def odd(number):\n",
    "    if (number % 2) == 0:\n",
    "        return False\n",
    "    else :\n",
    "        return True\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    id = comm.Get_rank()            #number of the process running the code\"\n",
    "    numProcesses = comm.Get_size()  #total number of processes running\"\n",
    "    myHostName = MPI.Get_processor_name()  #machine name running the code\"\n",
    "\n",
    "    # num of processes must be even\n",
    "    if numProcesses > 1 and not odd(numProcesses):\n",
    "        sendValue = id\n",
    "\n",
    "         #odd processes receive from their paired 'neighbor', then send\n",
    "        if odd(id):\n",
    "            comm.send(sendValue, dest=id - 1)\n",
    "            receivedValue = comm.recv(source=id - 1)\n",
    "\n",
    "        #even processes receive from their paired 'neighbor', then send\n",
    "        else:\n",
    "            receivedValue = comm.recv(source=id + 1)\n",
    "            comm.send(sendValue, dest=id + 1)\n",
    "\n",
    "        print(\n",
    "            \"Process {} of {} on {} computed {} and received {}\".format(\n",
    "                id, numProcesses, myHostName, sendValue, receivedValue\n",
    "            )\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        if id == 0:\n",
    "            print(\"Please run this program with a positive even number of processes.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "ad304f60e7ddd2c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task2.py\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T12:28:56.044654Z",
     "start_time": "2025-11-22T12:28:55.844543Z"
    }
   },
   "cell_type": "code",
   "source": "!mpiexec -n 6 python task2.py",
   "id": "d9d6f78e225c457c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 2 of 6 on DESKTOP-L6FRMTM computed 2 and received 3\n",
      "Process 4 of 6 on DESKTOP-L6FRMTM computed 4 and received 5\n",
      "\n",
      "Process 3 of 6 on DESKTOP-L6FRMTM computed 3 and received 2\n",
      "\n",
      "Process 5 of 6 on DESKTOP-L6FRMTM computed 5 and received 4\n",
      "\n",
      "\n",
      "Process 0 of 6 on DESKTOP-L6FRMTM computed 0 and received 1\n",
      "Process 1 of 6 on DESKTOP-L6FRMTM computed 1 and received 0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Як можемо бачити з цього прикладу, дані попарно відправляються між процесами один одному.\n",
    "\n",
    "Процес 1, відправив процесу 0 значення свого рангу \"1\", а процес 0 в свою чергу, відправив процесу 1 аналогічно - \"0\"\n",
    "\n",
    "**Атрибути функцій обміну (Send / Recv):**\n",
    "\n",
    "У блокуючому парному обміні беруть участь такі параметри:\n",
    "\n",
    "* buf - область памʼяті з даними для передачі або прийому.\n",
    "* count - кількість елементів у буфері.\n",
    "* datatype - MPI-тип одного елемента (MPI_INT, MPI_DOUBLE, масиви NumPy і т.д.).\n",
    "* dest / source - ранг процесу, якому надсилаємо або від якого очікуємо повідомлення.\n",
    "* tag - ціле число для логічної ідентифікації виду повідомлення.\n",
    "* comm - комунікатор (наприклад, MPI_COMM_WORLD).\n",
    "* status (тільки для Recv) - структура, у якій після завершення зберігається фактичний відправник, тег та код завершення.\n",
    "\n",
    "**Аналіз типів даних:**\n",
    "1. MPI не робить автоматичного перетворення типів: тип відправника і одержувача має збігатися.\n",
    "2. count і datatype однозначно визначають обсяг памʼяті, що MPI повинна передати.\n",
    "3. Якщо count = 0, операція дозволена й завершується миттєво.\n",
    "4. Розмір приймального буфера повинен бути не меншим, ніж фактичний розмір отриманих даних - інакше помилка.\n",
    "5. У mpi4py:\n",
    "  * send(obj) - обʼєкт серіалізується, тип визначається автоматично.\n",
    "  * Send([...], MPI.INT) - передаються сирі дані з чітким типом і розміром.\n",
    "6. Типи даних визначають спосіб розбору байтів, тому невідповідність типів призводить до некоректного читання або помилки.\n",
    "\n",
    "**Семантичний аналіз парного блокуючого обміну**\n",
    "\n",
    "1. Операція MPI_Send блокується, поки MPI не гарантує, що буфер можна змінювати (дані або передані, або скопійовані в буфер MPI).\n",
    "2. MPI_Recv блокується, поки дані не будуть повністю отримані і записані в приймальний буфер.\n",
    "3. Відправник та одержувач повинні бути логічно узгоджені: однаковий tag, однаковий datatype, однаковий count, коректно вказані source/dest\n",
    "4. Порушення симетрії веде до дедлоку (наприклад, обидва процеси одночасно викликають Send і жоден не виконує Recv).\n",
    "5. Завершення Send не означає, що одержувач вже отримав повідомлення - лише те, що відправник може використовувати буфер.\n",
    "6. Завершення Recv означає, що дані гарантовано у приймальному буфері.\n",
    "7. Парний блокуючий обмін створює локальну синхронізацію між процесами: один процес не може рухатись далі, доки пара не виконає узгоджену операцію."
   ],
   "id": "e09f029547399209"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Змінимо в програмі порядок виконання функцій для парних рангів, щоб продемострувати дедлок**",
   "id": "5f8db6a8bf22e086"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T12:28:56.059103Z",
     "start_time": "2025-11-22T12:28:56.054421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile task2_deadlock.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "# function to return whether a number of a process is odd or even\n",
    "def odd(number):\n",
    "    if (number % 2) == 0:\n",
    "        return False\n",
    "    else :\n",
    "        return True\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    id = comm.Get_rank()            #number of the process running the code\"\n",
    "    numProcesses = comm.Get_size()  #total number of processes running\"\n",
    "    myHostName = MPI.Get_processor_name()  #machine name running the code\"\n",
    "\n",
    "    # num of processes must be even\n",
    "    if numProcesses > 1 and not odd(numProcesses):\n",
    "        sendValue = id\n",
    "\n",
    "         #odd processes receive from their paired 'neighbor', then send\n",
    "        if odd(id):\n",
    "            receivedValue = comm.recv(source=id - 1)\n",
    "            comm.send(sendValue, dest=id - 1)\n",
    "\n",
    "        #even processes receive from their paired 'neighbor', then send\n",
    "        else:\n",
    "            receivedValue = comm.recv(source=id + 1)\n",
    "            comm.send(sendValue, dest=id + 1)\n",
    "\n",
    "        print(\n",
    "            \"Process {} of {} on {} computed {} and received {}\".format(\n",
    "                id, numProcesses, myHostName, sendValue, receivedValue\n",
    "            )\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        if id == 0:\n",
    "            print(\"Please run this program with a positive even number of processes.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "9de405c0c11948d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task2_deadlock.py\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T12:29:03.139724Z",
     "start_time": "2025-11-22T12:28:56.076329Z"
    }
   },
   "cell_type": "code",
   "source": "!mpiexec -n 6 python task2_deadlock.py",
   "id": "bcc1246995a25e02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Як бачимо процеси зависли - дедлок",
   "id": "37ada9b3a6d96fc2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**3. Виконати аналіз інших комунікаційних режимів. Показати зміни в шаблоні для їх реалізації.**",
   "id": "1a1690bf04598ced"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**У MPI існує чотири режими відправки повідомлень. Головна відмінність між ними — це умова завершення функції відправки (коли функція повертає управління програмі)**\n",
    "\n",
    "* Синхронний (Synchronous), ssend - Рукостискання. Функція завершується тільки тоді, коли процес-одержувач розпочав прийом повідомлення (викликав recv). Це гарантує, що обидва процеси досягли точки обміну. Найбезпечніший, але може бути повільнішим через очікування\n",
    "* Буферизований (Buffered), bsend - Через буфер користувача. Повідомлення копіюється у спеціально виділений  буфер, і функція повертається миттєво. Не треба залежати від того, чи готовий одержувач. Ризик: якщо буфер переповниться, виникне помилка.\n",
    "* По готовності (Ready), rsend - Оптимістичний. Можна викликати лише якщо ви впевнені на 100%, що одержувач вже викликав recv. Якщо ні — поведінка не визначена (помилка або краш). Використовується вкрай рідко для оптимізації."
   ],
   "id": "c91a31549407e721"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:03:04.530826Z",
     "start_time": "2025-11-22T13:03:04.524908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile task3.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "# Функція перевірки на непарність\n",
    "def odd(number):\n",
    "    return number % 2 != 0\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "    name = MPI.Get_processor_name()\n",
    "\n",
    "    # Перевірка, що кількість процесів парна\n",
    "    if size < 2 or odd(size):\n",
    "        if rank == 0:\n",
    "            print(\"Please run this program with a positive even number of processes.\")\n",
    "        exit(0)\n",
    "\n",
    "    # Підготовка даних (використовуємо numpy, як у вашому прикладі на фото)\n",
    "    # rank - це число, яке ми надсилаємо\n",
    "    send_data = np.array([rank], dtype='i')\n",
    "    recv_data = np.array([0], dtype='i')\n",
    "\n",
    "    # Визначаємо сусіда\n",
    "    if odd(rank):\n",
    "        neighbor = rank - 1\n",
    "    else:\n",
    "        neighbor = rank + 1\n",
    "\n",
    "    # === ЛОГІКА ОБМІНУ ===\n",
    "\n",
    "    # Для простоти реалізуємо схему:\n",
    "    # Непарні: ВІДПРАВЛЯЮТЬ всіма способами -> потім ПРИЙМАЮТЬ всіма способами\n",
    "    # Парні:   ПРИЙМАЮТЬ всіма способами   -> потім ВІДПРАВЛЯЮТЬ всіма способами\n",
    "    # Це запобігає дедлокам і дозволяє Rsend працювати коректно (бо Recv вже викликаний).\n",
    "\n",
    "    if odd(rank):\n",
    "        # --- 1. ВІДПРАВКА (Send) ---\n",
    "        # Standard\n",
    "        comm.Send([send_data, MPI.INT], dest=neighbor, tag=0)\n",
    "\n",
    "        # Buffered\n",
    "        buf = bytearray(MPI.BSEND_OVERHEAD + send_data.nbytes)\n",
    "        MPI.Attach_buffer(buf)\n",
    "        comm.Bsend([send_data, MPI.INT], dest=neighbor, tag=1)\n",
    "        MPI.Detach_buffer()\n",
    "\n",
    "        # Synchronous\n",
    "        comm.Ssend([send_data, MPI.INT], dest=neighbor, tag=2)\n",
    "\n",
    "        # Ready (працює, бо парний процес вже чекає на Recv)\n",
    "        comm.Rsend([send_data, MPI.INT], dest=neighbor, tag=3)\n",
    "\n",
    "        print(f\"Process {rank} finished ALL SENDS to {neighbor}\")\n",
    "\n",
    "        # --- 2. ПРИЙОМ (Receive) ---\n",
    "        comm.Recv([recv_data, MPI.INT], source=neighbor, tag=0) # Standard\n",
    "        comm.Recv([recv_data, MPI.INT], source=neighbor, tag=1) # Buffered\n",
    "        comm.Recv([recv_data, MPI.INT], source=neighbor, tag=2) # Sync\n",
    "        comm.Recv([recv_data, MPI.INT], source=neighbor, tag=3) # Ready\n",
    "\n",
    "        print(f\"Process {rank} finished ALL RECVS from {neighbor}. Last value: {recv_data[0]}\")\n",
    "\n",
    "    else:\n",
    "        # --- 1. ПРИЙОМ (Receive) ---\n",
    "        # Парні процеси спочатку слухають, що робить їх готовими до Rsend від сусідів\n",
    "        comm.Recv([recv_data, MPI.INT], source=neighbor, tag=0) # Standard\n",
    "        comm.Recv([recv_data, MPI.INT], source=neighbor, tag=1) # Buffered\n",
    "        comm.Recv([recv_data, MPI.INT], source=neighbor, tag=2) # Sync\n",
    "        comm.Recv([recv_data, MPI.INT], source=neighbor, tag=3) # Ready\n",
    "\n",
    "        print(f\"Process {rank} finished ALL RECVS from {neighbor}. Last value: {recv_data[0]}\")\n",
    "\n",
    "        # --- 2. ВІДПРАВКА (Send) ---\n",
    "        # Standard\n",
    "        comm.Send([send_data, MPI.INT], dest=neighbor, tag=0)\n",
    "\n",
    "        # Buffered\n",
    "        buf = bytearray(MPI.BSEND_OVERHEAD + send_data.nbytes)\n",
    "        MPI.Attach_buffer(buf)\n",
    "        comm.Bsend([send_data, MPI.INT], dest=neighbor, tag=1)\n",
    "        MPI.Detach_buffer()\n",
    "\n",
    "        # Synchronous\n",
    "        comm.Ssend([send_data, MPI.INT], dest=neighbor, tag=2)\n",
    "\n",
    "        # Ready\n",
    "        comm.Rsend([send_data, MPI.INT], dest=neighbor, tag=3)\n",
    "\n",
    "        print(f\"Process {rank} finished ALL SENDS to {neighbor}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "7768fbb394373177",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task3.py\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:03:16.951293Z",
     "start_time": "2025-11-22T13:03:16.644106Z"
    }
   },
   "cell_type": "code",
   "source": "!mpiexec -n 6 python task3.py",
   "id": "ac975ba6f097c6fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 1 finished ALL SENDS to 0\n",
      "Process 0 finished ALL RECVS from 1. Last value: 1\n",
      "\n",
      "\n",
      "Process 0 finished ALL SENDS to 1\n",
      "Process 1 finished ALL RECVS from 0. Last value: 0\n",
      "\n",
      "\n",
      "Process 5 finished ALL SENDS to 4\n",
      "Process 4 finished ALL RECVS from 5. Last value: 5\n",
      "\n",
      "\n",
      "Process 4 finished ALL SENDS to 5\n",
      "Process 5 finished ALL RECVS from 4. Last value: 4\n",
      "\n",
      "\n",
      "Process 3 finished ALL SENDS to 2\n",
      "Process 2 finished ALL RECVS from 3. Last value: 3\n",
      "\n",
      "\n",
      "Process 2 finished ALL SENDS to 3\n",
      "Process 3 finished ALL RECVS from 2. Last value: 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4. Сутність та аналіз функцій парного неблокуючого обміну**\n",
    "\n",
    "Сутність неблокуючого обміну Неблокуючий обмін - це техніка, яка дозволяє процесу ініціювати операцію передачі або прийому даних і негайно продовжити виконання програми, не чекаючи фактичного завершення цієї операції. На відміну від блокуючого режиму, де процес \"зависає\" до моменту, поки дані не будуть безпечно скопійовані, неблокуючий режим розділяє комунікацію на дві фази:\n",
    "\n",
    "1. Ініціалізація: Виклик функцій початку обміну (MPI_Isend, MPI_Irecv). Ці функції лише повідомляють системі про намір передати/прийняти дані та миттєво повертають керування програмі, повертаючи спеціальний об'єкт - запит (Request).\n",
    "2. Завершення: Перевірка стану операції за допомогою отриманого об'єкта запиту. Для цього використовуються функції очікування MPI_Wait (блокує процес до завершення) або перевірки MPI_Test (повертає статус, не блокуючи процес)."
   ],
   "id": "f1e1530eae89f505"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Аналіз функцій**\n",
    "\n",
    "MPI_Isend / comm.isend: Ініціює відправку. Буфер з даними не можна змінювати після цього виклику до моменту успішного завершення операції (виклику Wait), оскільки система може все ще читати з нього дані.\n",
    "\n",
    "MPI_Irecv / comm.irecv: Ініціює прийом. Спроба прочитати дані з буфера до завершення операції призведе до отримання сміття, оскільки дані могли ще не надійти.\n",
    "\n",
    "MPI_Wait / req.wait: Гарантує завершення операції. Повернення з цієї функції означає, що буфер відправника можна перезаписувати, або що в буфері одержувача вже лежать коректні дані."
   ],
   "id": "bdbe4e454eda5a88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:09:07.215134Z",
     "start_time": "2025-11-22T13:09:07.210093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile task4.py\n",
    "from mpi4py import MPI\n",
    "import time\n",
    "\n",
    "def odd(number):\n",
    "    return number % 2 != 0\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "    name = MPI.Get_processor_name()\n",
    "\n",
    "    # Перевірка на парність кількості процесів\n",
    "    if size < 2 or odd(size):\n",
    "        if rank == 0:\n",
    "            print(\"Error: Please run with an even number of processes.\")\n",
    "        return\n",
    "\n",
    "    # Визначаємо сусіда\n",
    "    neighbor = rank - 1 if odd(rank) else rank + 1\n",
    "\n",
    "    send_val = rank\n",
    "    print(f\"[P{rank}] Initialized. Prepare to exchange with P{neighbor}\")\n",
    "\n",
    "    # === НЕБЛОКУЮЧИЙ ОБМІН ===\n",
    "\n",
    "    # 1. Ініціалізація ПРИЙОМУ (Irecv)\n",
    "    # Функція повертає об'єкт Request, але не чекає даних\n",
    "    print(f\"[P{rank}] Starting non-blocking Receive...\")\n",
    "    req_recv = comm.irecv(source=neighbor, tag=11)\n",
    "\n",
    "    # 2. Ініціалізація ВІДПРАВКИ (Isend)\n",
    "    # Функція повертає об'єкт Request і миттєво йде далі\n",
    "    print(f\"[P{rank}] Starting non-blocking Send ({send_val})...\")\n",
    "    req_send = comm.isend(send_val, dest=neighbor, tag=11)\n",
    "\n",
    "    # 3. КОРИСНА РОБОТА (Overlapping)\n",
    "    # У цей час, поки дані \"летять\" по мережі, процесор не простоює.\n",
    "    # Ми можемо виконувати обчислення.\n",
    "    print(f\"[P{rank}] Doing some calculations while waiting...\")\n",
    "    time.sleep(1) # Імітація обчислень\n",
    "\n",
    "    # 4. ОЧІКУВАННЯ ЗАВЕРШЕННЯ (Wait)\n",
    "    # Тепер нам потрібні результати, тому ми явно чекаємо завершення.\n",
    "\n",
    "    # Чекаємо, поки відправка точно завершиться (буфер стане вільним)\n",
    "    req_send.wait()\n",
    "    print(f\"[P{rank}] Send completed.\")\n",
    "\n",
    "    # Чекаємо, поки дані точно прийдуть\n",
    "    received_val = req_recv.wait()\n",
    "    print(f\"[P{rank}] Receive completed. Got data: {received_val}\")\n",
    "\n",
    "    # Фінальний вивід\n",
    "    print(f\"SUCCESS: Process {rank} on {name} | Sent: {send_val} | Received: {received_val}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "798bd571e6cafac8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task4.py\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:09:13.800786Z",
     "start_time": "2025-11-22T13:09:12.576498Z"
    }
   },
   "cell_type": "code",
   "source": "!mpiexec -n 6 python task4.py",
   "id": "9502ed58d4753774",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P3] Initialized. Prepare to exchange with P2\n",
      "[P3] Starting non-blocking Receive...\n",
      "[P5] Initialized. Prepare to exchange with P4\n",
      "[P5] Starting non-blocking Receive...\n",
      "[P2] Initialized. Prepare to exchange with P3\n",
      "[P2] Starting non-blocking Receive...\n",
      "[P4] Initialized. Prepare to exchange with P5\n",
      "[P4] Starting non-blocking Receive...\n",
      "[P3] Starting non-blocking Send (3)...\n",
      "[P5] Starting non-blocking Send (5)...\n",
      "[P2] Starting non-blocking Send (2)...\n",
      "[P4] Starting non-blocking Send (4)...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[P0] Initialized. Prepare to exchange with P1\n",
      "[P1] Initialized. Prepare to exchange with P0\n",
      "[P0] Starting non-blocking Receive...\n",
      "\n",
      "\n",
      "[P1] Starting non-blocking Receive...\n",
      "[P0] Starting non-blocking Send (0)...\n",
      "\n",
      "\n",
      "[P4] Doing some calculations while waiting...\n",
      "[P5] Doing some calculations while waiting...\n",
      "[P2] Doing some calculations while waiting...\n",
      "[P3] Doing some calculations while waiting...\n",
      "[P1] Starting non-blocking Send (1)...\n",
      "[P0] Doing some calculations while waiting...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[P1] Doing some calculations while waiting...\n",
      "[P0] Send completed.\n",
      "[P3] Send completed.\n",
      "[P2] Send completed.\n",
      "[P4] Send completed.\n",
      "[P1] Send completed.\n",
      "[P5] Send completed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[P0] Receive completed. Got data: 1\n",
      "[P3] Receive completed. Got data: 2\n",
      "[P2] Receive completed. Got data: 3\n",
      "[P4] Receive completed. Got data: 5\n",
      "[P5] Receive completed. Got data: 4\n",
      "[P1] Receive completed. Got data: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SUCCESS: Process 0 on DESKTOP-L6FRMTM | Sent: 0 | Received: 1\n",
      "SUCCESS: Process 3 on DESKTOP-L6FRMTM | Sent: 3 | Received: 2\n",
      "SUCCESS: Process 2 on DESKTOP-L6FRMTM | Sent: 2 | Received: 3\n",
      "SUCCESS: Process 4 on DESKTOP-L6FRMTM | Sent: 4 | Received: 5\n",
      "SUCCESS: Process 5 on DESKTOP-L6FRMTM | Sent: 5 | Received: 4\n",
      "SUCCESS: Process 1 on DESKTOP-L6FRMTM | Sent: 1 | Received: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**5. Побудувати програму-шаблон для парного обміну списками даних.**\n",
    "\n",
    "У бібліотеці mpi4py обмін складними типами даних (такими як списки, словники, кортежі) реалізований дуже просто завдяки механізму серіалізації (pickle).\n",
    "\n",
    "* Для передачі списків використовуються функції: comm.send та comm.recv.\n",
    "* Не потрібно вказувати тип даних (MPI.INT, MPI.FLOAT тощо) або розмір буфера, як це робиться для масивів NumPy. MPI сама визначить структуру списку, запакує його і передасть."
   ],
   "id": "865cebf7fb32966c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:12:52.841076Z",
     "start_time": "2025-11-22T13:12:52.836332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile task5.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "def odd(number):\n",
    "    return number % 2 != 0\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "    name = MPI.Get_processor_name()\n",
    "\n",
    "    # Перевірка на парність кількості процесів\n",
    "    if size < 2 or odd(size):\n",
    "        if rank == 0:\n",
    "            print(\"Error: Please run with an even number of processes.\")\n",
    "        return\n",
    "\n",
    "    # Визначаємо сусіда (партнера по обміну)\n",
    "    neighbor = rank - 1 if odd(rank) else rank + 1\n",
    "\n",
    "    # === 1. СТВОРЕННЯ СПИСКУ ДАНИХ ===\n",
    "    # Створимо список, що містить різні типи даних:\n",
    "    # [номер_процесу, математичне значення, рядок тексту]\n",
    "    my_list = [rank, rank * 7, f\"Hello from P{rank}\"]\n",
    "\n",
    "    print(f\"[Process {rank}] Created list: {my_list}\")\n",
    "\n",
    "    # === 2. ПАРНИЙ ОБМІН СПИСКАМИ ===\n",
    "    # Використовуємо блокуючий обмін (send/recv) для Python-об'єктів\n",
    "\n",
    "    if odd(rank):\n",
    "        # Непарний: Відправляє -> Приймає\n",
    "        comm.send(my_list, dest=neighbor, tag=50)\n",
    "        received_list = comm.recv(source=neighbor, tag=50)\n",
    "    else:\n",
    "        # Парний: Приймає -> Відправляє\n",
    "        received_list = comm.recv(source=neighbor, tag=50)\n",
    "        comm.send(my_list, dest=neighbor, tag=50)\n",
    "\n",
    "    # === 3. ВИВІД РЕЗУЛЬТАТУ ===\n",
    "    # Ми отримали повноцінний Python-список, з яким можна працювати далі\n",
    "    print(f\"SUCCESS: Process {rank} received list: {received_list} (Type: {type(received_list)})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "8cf6500dd0d13014",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task5.py\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:12:59.701787Z",
     "start_time": "2025-11-22T13:12:59.492150Z"
    }
   },
   "cell_type": "code",
   "source": "!mpiexec -n 6 python task5.py",
   "id": "aa68e0cf1a0c7af5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Process 1] Created list: [1, 7, 'Hello from P1']\n",
      "[Process 0] Created list: [0, 0, 'Hello from P0']\n",
      "[Process 2] Created list: [2, 14, 'Hello from P2']\n",
      "[Process 3] Created list: [3, 21, 'Hello from P3']\n",
      "SUCCESS: Process 0 received list: [1, 7, 'Hello from P1'] (Type: <class 'list'>)\n",
      "[Process 5] Created list: [5, 35, 'Hello from P5']\n",
      "\n",
      "\n",
      "SUCCESS: Process 1 received list: [0, 0, 'Hello from P0'] (Type: <class 'list'>)\n",
      "SUCCESS: Process 2 received list: [3, 21, 'Hello from P3'] (Type: <class 'list'>)\n",
      "SUCCESS: Process 3 received list: [2, 14, 'Hello from P2'] (Type: <class 'list'>)\n",
      "[Process 4] Created list: [4, 28, 'Hello from P4']\n",
      "SUCCESS: Process 4 received list: [5, 35, 'Hello from P5'] (Type: <class 'list'>)\n",
      "SUCCESS: Process 5 received list: [4, 28, 'Hello from P4'] (Type: <class 'list'>)\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "6. Побудувати програму-шаблон для кільцевого парного обміну.\n",
    "\n",
    "Суть алгоритму. Це класична топологія Кільце (Ring).\n",
    "\n",
    "1. Процес 0 створює список і відправляє його сусіду праворуч (Rank 1).\n",
    "2. Кожен наступний процес отримує список, додає до нього свій номер (Rank) і передає далі по колу.\n",
    "3. Останній процес передає список назад Процесу 0, замикаючи кільце."
   ],
   "id": "ebbdb49e39a9d980"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:16:15.783822Z",
     "start_time": "2025-11-22T13:16:15.778358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile task6.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    id = comm.Get_rank()            # Номер поточного процесу\n",
    "    numProcesses = comm.Get_size()  # Загальна кількість процесів\n",
    "    myHostName = MPI.Get_processor_name()\n",
    "\n",
    "    if numProcesses > 1:\n",
    "        # --- ЛОГІКА МАЙСТРА (Початок і кінець кільця) ---\n",
    "        if id == 0:\n",
    "            # 1. Створюємо початковий список\n",
    "            sendList = [id]\n",
    "\n",
    "            # 2. Відправляємо першому робочому процесу (id + 1)\n",
    "            comm.send(sendList, dest=id + 1)\n",
    "            print(f\"Master Process {id} on {myHostName} started ring with {sendList}\")\n",
    "\n",
    "            # 3. Чекаємо повернення списку від останнього процесу\n",
    "            receivedList = comm.recv(source=numProcesses - 1)\n",
    "            print(f\"Master Process {id} on {myHostName} received final list: {receivedList}\")\n",
    "\n",
    "        # --- ЛОГІКА РОБОЧИХ ПРОЦЕСІВ (Ланки кільця) ---\n",
    "        else:\n",
    "            # 1. Отримуємо список від попереднього процесу (id - 1)\n",
    "            receivedList = comm.recv(source=id - 1)\n",
    "\n",
    "            # 2. Додаємо свій номер до списку\n",
    "            sendList = receivedList + [id]\n",
    "\n",
    "            # 3. Відправляємо далі по колу.\n",
    "            # Формула (id + 1) % numProcesses забезпечує, що останній процес відправить 0-му.\n",
    "            dest = (id + 1) % numProcesses\n",
    "            comm.send(sendList, dest=dest)\n",
    "\n",
    "            print(f\"Worker Process {id} on {myHostName} received {receivedList} and sent {sendList}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Please run this program with the number of processes greater than 1\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "b4d36ce8da520a4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task6.py\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:16:27.299567Z",
     "start_time": "2025-11-22T13:16:27.085228Z"
    }
   },
   "cell_type": "code",
   "source": "!mpiexec -n 6 python task6.py",
   "id": "fc46210b09577573",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Process 0 on DESKTOP-L6FRMTM started ring with [0]\n",
      "Worker Process 1 on DESKTOP-L6FRMTM received [0] and sent [0, 1]\n",
      "Worker Process 2 on DESKTOP-L6FRMTM received [0, 1] and sent [0, 1, 2]\n",
      "Worker Process 3 on DESKTOP-L6FRMTM received [0, 1, 2] and sent [0, 1, 2, 3]\n",
      "Worker Process 4 on DESKTOP-L6FRMTM received [0, 1, 2, 3] and sent [0, 1, 2, 3, 4]\n",
      "Worker Process 5 on DESKTOP-L6FRMTM received [0, 1, 2, 3, 4] and sent [0, 1, 2, 3, 4, 5]\n",
      "Master Process 0 on DESKTOP-L6FRMTM received final list: [0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "7. Побудувати та виконати програму розподілених обчислень у вибраному середовищі з використаннм побудованих шаблонів для тестової задачі.\n",
    "\n",
    "Логіка програми\n",
    "1. Майстер (Rank 0) генерує масив.\n",
    "2. Розподіл (Distribute): Майстер у циклі відправляє шматки масиву іншим процесам, використовуючи блокуючий обмін буферами.\n",
    "3. Обчислення: Всі процеси (і Майстер, і Worker-и) рахують свої локальні значення.\n",
    "4. Збір (Collect): Worker-и пакують результати (суму, суму квадратів, гістограму) у Python-список/словник і відправляють Майстру. Тут використовується обмін списками, бо це зручно для складних структур."
   ],
   "id": "50baba70eea39598"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:32:32.073649Z",
     "start_time": "2025-11-22T13:32:32.067126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile task7.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "\n",
    "    N_VARIANT = 7               # номер варіанту\n",
    "    TOTAL_ITEMS = 1000 * N_VARIANT\n",
    "    MIN_VAL, MAX_VAL = 0, 100   # Діапазон чисел\n",
    "    BINS = 10                   # Кількість стовпчиків гістограми\n",
    "\n",
    "    # Розрахунок розміру шматка даних для одного процесу\n",
    "    chunk_size = TOTAL_ITEMS // size\n",
    "\n",
    "    # Змінна для локальних даних\n",
    "    local_data = np.empty(chunk_size, dtype='i')\n",
    "\n",
    "    # ЕТАП 1: РОЗСИЛКА ДАНИХ (БЛОКУЮЧИЙ ОБМІН)\n",
    "    if rank == 0:\n",
    "        print(f\"--- START: Generating {TOTAL_ITEMS} numbers for {size} processes ---\")\n",
    "\n",
    "        # Генеруємо повний масив (тільки на Master)\n",
    "        full_data = np.random.randint(MIN_VAL, MAX_VAL + 1, size=TOTAL_ITEMS, dtype='i')\n",
    "\n",
    "        # 1. Відправляємо порції іншим процесам\n",
    "        for i in range(1, size):\n",
    "            # Вирізаємо шматок\n",
    "            start = i * chunk_size\n",
    "            end = start + chunk_size\n",
    "            data_to_send = full_data[start:end]\n",
    "\n",
    "            # Використовуємо Send (блокуючий, для буферів)\n",
    "            comm.Send([data_to_send, MPI.INT], dest=i, tag=10)\n",
    "\n",
    "        # 2. Залишаємо собі перший шматок\n",
    "        local_data = full_data[0:chunk_size]\n",
    "        print(\"[Master] Data distributed via blocking Send.\")\n",
    "\n",
    "    else:\n",
    "        # Worker-и чекають на дані\n",
    "        # Використовуємо Recv (блокуючий)\n",
    "        comm.Recv([local_data, MPI.INT], source=0, tag=10)\n",
    "\n",
    "    # ЕТАП 2: ЛОКАЛЬНІ ОБЧИСЛЕННЯ (ПАРАЛЕЛЬНО)\n",
    "    # 1. Локальна сума\n",
    "    loc_sum = np.sum(local_data)\n",
    "\n",
    "    # 2. Локальна сума квадратів (потрібна для дисперсії)\n",
    "    # Конвертуємо в int64, щоб уникнути переповнення\n",
    "    loc_sq_sum = np.sum(local_data.astype(np.int64) ** 2)\n",
    "\n",
    "    # 3. Локальна гістограма\n",
    "    loc_hist, _ = np.histogram(local_data, bins=BINS, range=(MIN_VAL, MAX_VAL))\n",
    "\n",
    "    loc_count = len(local_data)\n",
    "\n",
    "    comm.Barrier() # Синхронізуємо всі процеси перед стартом\n",
    "    start_time = MPI.Wtime()\n",
    "\n",
    "    # ЕТАП 3: ЗБІР РЕЗУЛЬТАТІВ (ПАТЕРН: ОБМІН СПИСКАМИ ДАНИХ)\n",
    "    if rank == 0:\n",
    "        # Ініціалізуємо загальні змінні результатами майстра\n",
    "        total_sum = loc_sum\n",
    "        total_sq_sum = loc_sq_sum\n",
    "        total_hist = loc_hist\n",
    "        total_count = loc_count\n",
    "\n",
    "        print(\"[Master] Collecting results via List exchange...\")\n",
    "\n",
    "        # У циклі приймаємо пакети від кожного Worker-а\n",
    "        for i in range(1, size):\n",
    "            # Використовуємо recv (мала літера) для отримання словника/списку\n",
    "            result_pkg = comm.recv(source=i, tag=20)\n",
    "\n",
    "            # Розпаковка та агрегація\n",
    "            total_sum += result_pkg['s']\n",
    "            total_sq_sum += result_pkg['sq']\n",
    "            total_hist += result_pkg['h']\n",
    "            total_count += result_pkg['c']\n",
    "\n",
    "        # ЕТАП 4: ФІНАЛЬНА СТАТИСТИКА\n",
    "\n",
    "        # Середнє\n",
    "        mean_val = total_sum / total_count\n",
    "\n",
    "        # Стандартне відхилення\n",
    "        # Var = Mean(X^2) - (Mean(X))^2\n",
    "        variance = (total_sq_sum / total_count) - (mean_val ** 2)\n",
    "        std_dev = np.sqrt(variance)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\" FINAL RESULTS (N={N_VARIANT})\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"Total Elements: {total_count}\")\n",
    "        print(f\"Mean:           {mean_val:.4f}\")\n",
    "        print(f\"Std Deviation:  {std_dev:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"Histogram:\")\n",
    "        print(total_hist)\n",
    "\n",
    "        # Візуалізація гістограми (текстова)\n",
    "        print(\"-\" * 40)\n",
    "        max_h = max(total_hist)\n",
    "        for idx, val in enumerate(total_hist):\n",
    "            bar_len = int((val / max_h) * 20)\n",
    "            range_lbl = f\"{idx*10}-{(idx+1)*10}\"\n",
    "            print(f\"{range_lbl:7} : {'#' * bar_len} ({val})\")\n",
    "\n",
    "    else:\n",
    "        # Worker пакує результати у словник (зручніше ніж просто список)\n",
    "        result_pkg = {\n",
    "            's': loc_sum,\n",
    "            'sq': loc_sq_sum,\n",
    "            'h': loc_hist,\n",
    "            'c': loc_count\n",
    "        }\n",
    "        # Використовуємо send (мала літера) для відправки об'єкта\n",
    "        comm.send(result_pkg, dest=0, tag=20)\n",
    "\n",
    "    # КІНЕЦЬ ВИМІРУ ЧАСУ\n",
    "    # ==========================================\n",
    "    comm.Barrier() # Чекаємо, поки всі закінчать\n",
    "    end_time = MPI.Wtime()\n",
    "    if rank == 0:\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Processes: {size} | Time: {execution_time:.6f} sec\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "59a3f20c569be78b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task7.py\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:56:16.209687Z",
     "start_time": "2025-11-22T13:56:15.240535Z"
    }
   },
   "cell_type": "code",
   "source": "!mpiexec -n 7 python task7.py",
   "id": "be1c866d9b371588",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- START: Generating 7000 numbers for 7 processes ---\n",
      "[Master] Data distributed via blocking Send.\n",
      "[Master] Collecting results via List exchange...\n",
      "\n",
      "========================================\n",
      " FINAL RESULTS (N=7)\n",
      "========================================\n",
      "Total Elements: 7000\n",
      "Mean:           50.1970\n",
      "Std Deviation:  29.3547\n",
      "----------------------------------------\n",
      "Histogram:\n",
      "[690 681 734 684 646 720 650 677 734 784]\n",
      "----------------------------------------\n",
      "0-10    : ################# (690)\n",
      "10-20   : ################# (681)\n",
      "20-30   : ################## (734)\n",
      "30-40   : ################# (684)\n",
      "40-50   : ################ (646)\n",
      "50-60   : ################## (720)\n",
      "60-70   : ################ (650)\n",
      "70-80   : ################# (677)\n",
      "80-90   : ################## (734)\n",
      "90-100  : #################### (784)\n",
      "Processes: 7 | Time: 0.006134 sec\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:55:22.942953Z",
     "start_time": "2025-11-22T13:55:22.580234Z"
    }
   },
   "cell_type": "code",
   "source": "!mpiexec -n 4 python task7.py",
   "id": "f3f53d1d27ee3094",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- START: Generating 7000 numbers for 4 processes ---\n",
      "[Master] Data distributed via blocking Send.\n",
      "[Master] Collecting results via List exchange...\n",
      "\n",
      "========================================\n",
      " FINAL RESULTS (N=7)\n",
      "========================================\n",
      "Total Elements: 7000\n",
      "Mean:           50.2043\n",
      "Std Deviation:  29.1315\n",
      "----------------------------------------\n",
      "Histogram:\n",
      "[699 665 676 696 696 718 685 718 678 769]\n",
      "----------------------------------------\n",
      "0-10    : ################## (699)\n",
      "10-20   : ################# (665)\n",
      "20-30   : ################# (676)\n",
      "30-40   : ################## (696)\n",
      "40-50   : ################## (696)\n",
      "50-60   : ################## (718)\n",
      "60-70   : ################# (685)\n",
      "70-80   : ################## (718)\n",
      "80-90   : ################# (678)\n",
      "90-100  : #################### (769)\n",
      "Processes: 4 | Time: 0.001521 sec\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:55:34.108741Z",
     "start_time": "2025-11-22T13:55:33.832214Z"
    }
   },
   "cell_type": "code",
   "source": "!mpiexec -n 1 python task7.py",
   "id": "8746923b2a266c58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- START: Generating 7000 numbers for 1 processes ---\n",
      "[Master] Data distributed via blocking Send.\n",
      "[Master] Collecting results via List exchange...\n",
      "\n",
      "========================================\n",
      " FINAL RESULTS (N=7)\n",
      "========================================\n",
      "Total Elements: 7000\n",
      "Mean:           49.5530\n",
      "Std Deviation:  29.0607\n",
      "----------------------------------------\n",
      "Histogram:\n",
      "[701 702 694 710 722 688 692 665 693 733]\n",
      "----------------------------------------\n",
      "0-10    : ################### (701)\n",
      "10-20   : ################### (702)\n",
      "20-30   : ################## (694)\n",
      "30-40   : ################### (710)\n",
      "40-50   : ################### (722)\n",
      "50-60   : ################## (688)\n",
      "60-70   : ################## (692)\n",
      "70-80   : ################## (665)\n",
      "80-90   : ################## (693)\n",
      "90-100  : #################### (733)\n",
      "Processes: 1 | Time: 0.000980 sec\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Результати виконання показали, що на одному процесі час становить\n",
    "* T1 = 0.000980 s\n",
    "\n",
    "Час виконання на 4 та 6 процесах становив відповідно\n",
    "* T4 = 0.001521 s\n",
    "* T7 = 0.006134 s\n",
    "\n",
    "Експериментальне прискорення обчислюється відношенням часу на одному процесі до часу на N процесах. Для чотирьох процесів воно дорівнює\n",
    "* 𝑆𝑒𝑥𝑝(4) = T1/T4 = 0.64431\n",
    "\n",
    "Для шести процесів аналогічні обчислення дають\n",
    "* 𝑆𝑒𝑥𝑝(7) = 0.1597\n",
    "\n",
    "Експериментальна ефективність визначається як відношення прискорення до кількості процесів. Для чотирьох процесів ефективність становить\n",
    "\n",
    "* 𝐸𝑒𝑥𝑝(4) = 0.64431 / 4 ≈ 0.1611\n",
    "* 𝐸𝑒𝑥𝑝(7) = 0.1597 / 7 ≈ 0.02282\n",
    "\n",
    "За законом Амдала теоретичне прискорення визначається формулою\n",
    "\n",
    "![image.png](attachment:89e73aa4-30b7-4a96-9110-4b46e1328845.png)\n",
    "\n",
    "де a - частка програми, що може бути розпаралелена.\n",
    "\n",
    "Розвязавши рівняння з експериментальним значенням 𝑆𝑒𝑥𝑝(7) = 0.1597 приходимо висновку, що розпаралелити можна близько 5% коду, що в даному випадку не має сенсу, через занадто малу вибірку даних."
   ],
   "id": "b51f0f1402407191",
   "attachments": {
    "c566427e-33dc-4595-a260-3cba8cf4650e.png": {
     "image/png": "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"
    },
    "d99e2f52-db94-494e-bd3b-8875c15b317d.png": {
     "image/png": "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"
    },
    "3ec68aff-226b-4335-86ea-57ed9e11e2bc.png": {
     "image/png": "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"
    },
    "89e73aa4-30b7-4a96-9110-4b46e1328845.png": {
     "image/png": "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"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Висновки:**\n",
    "\n",
    "У ході виконання лабораторної роботи було досліджено принципи та механізми парного обміну в розподілених обчислювальних системах з використанням бібліотеки MPI. Було розглянуто стандартні, синхронні, буферизовані та режими готовності, виконано їх аналіз та порівняння. На основі отриманих знань побудовано шаблони програм для парного блокуючого, неблокуючого, кільцевого та спискового обміну даними. У процесі роботи було з’ясовано:\n",
    "\n",
    "Парний обмін є базовим механізмом організації взаємодії процесів у системах з розподіленою памʼяттю. Функції MPI_Send/MPI_Recv у блокуючому режимі забезпечують коректність обміну, але можуть призводити до дедлоків за неправильного упорядкування викликів.  Неблокуючі функції MPI_Isend/MPI_Irecv підвищують потенційну продуктивність, дозволяючи перекривати обчислення і передачу даних, але вимагають додаткового контролю завершення операцій.  Режими комунікації відрізняються способом синхронізації та буферизації, що впливає на швидкодію та можливі ситуації блокування.  Реалізація парного обміну списками та кільцевого парного обміну дозволила зрозуміти особливості передавання складних структур даних між процесами. Для тестової задачі було побудовано розподілене обчислення гістограми, середнього значення та середньоквадратичного відхилення послідовності випадкових чисел, що дало змогу оцінити ефективність паралельної програми.\n"
   ],
   "id": "9662cff5f402e5ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Контрольні запитання**\n",
    "\n",
    "1. Що таке атрибути повідомлення?\n",
    "\n",
    "Атрибути повідомлення в MPI — це службова інформація, яка супроводжує власне дані і дозволяє комунікаційній системі розпізнавати та правильно доставляти повідомлення. Цю сукупність атрибутів часто називають «конвертом» повідомлення, і до неї обов'язково входять номер процесу-відправника (source rank), номер процесу-одержувача (destination rank), тег повідомлення (ідентифікатор для розрізнення повідомлень) та комунікатор (група процесів, в межах якої відбувається обмін).\n",
    "\n",
    "2. Чи повертає функція MPI_Send код помилки, номер процесу, якому адресовано передачу?\n",
    "\n",
    "Функція MPI_Send повертає лише код виконання операції, який зазвичай дорівнює константі MPI_SUCCESS у разі успіху або коду помилки у разі збою. Вона не повертає номер процесу, якому адресовано передачу, оскільки цей параметр (destination rank) є вхідним аргументом функції, тобто він вже відомий програмісту в момент виклику.\n",
    "\n",
    "3. Чи відбувається повернення з функції MPI_Send, коли можна повторно використовувати параметри цієї функції або коли повідомлення залишить процес, або коли повідомлення прийнято адресатом або коли адресат ініціював прийом цього повідомлення?\n",
    "\n",
    "Повернення з функції MPI_Send у стандартному режимі відбувається тоді, коли буфер відправлення (змінні, передані як параметри) можна безпечно використовувати повторно, тобто змінювати в ньому дані без ризику зіпсувати повідомлення, що відправляється. Це може означати, що повідомлення було скопійовано у системний буфер і функція повернулася, або ж (якщо буферизація неможлива) функція заблокувалася до моменту, поки адресат не почав прийом і дані не вийшли з процесу-відправника.\n",
    "\n",
    "4. Чи може значення змінної count дорівнювати нулю?\n",
    "\n",
    "Так, значення змінної count може дорівнювати нулю. Таке повідомлення називається «порожнім» і містить лише атрибути (конверт) без тіла даних; воно часто використовується для цілей синхронізації процесів, коли сам факт отримання повідомлення є сигналом, а передача даних не потрібна.\n",
    "\n",
    "5. Чи можна як значення тега в команді посилки передати значення але міра процесу в комунікаторі?\n",
    "\n",
    "Так, як значення тега можна використовувати число, що дорівнює номеру (рангу) процесу в комунікаторі, або будь-яке інше ціле невід'ємне число в межах, визначених реалізацією MPI. Тег — це просто ідентифікатор, і якщо логіка програми передбачає використання рангу процесу як тега для зручності, це є цілком допустимим.\n",
    "\n",
    "6. Чи може довжина буфера одержувача бути більшою, ніж довжина прийнятого повідомлення? А меншою?\n",
    "\n",
    "Довжина буфера одержувача може бути більшою за довжину реального повідомлення; у такому разі частина буфера просто залишиться невикористаною, і це не вважається помилкою. Однак вона не може бути меншою, оскільки спроба прийняти повідомлення, яке більше за виділений буфер, призведе до помилки переповнення (overflow error) і аварійного завершення програми або повернення коду помилки.\n",
    "\n",
    "7. Який діапазон значень, що приймаються константами MPI_ANY_SOURCE, MPI_ANY_TAG?\n",
    "\n",
    "Константи MPI_ANY_SOURCE та MPI_ANY_TAG зазвичай мають від'ємні значення або спеціальні значення, які знаходяться поза діапазоном допустимих рангів процесів (від 0 до N-1) та звичайних тегів. Це дозволяє функції MPI_Recv однозначно відрізняти їх від конкретних ідентифікаторів та переходити в режим прийому від будь-якого джерела або з будь-яким тегом.\n",
    "\n",
    "8. Чи можна використовувати операції MPI_ANY_SOURCE в операції? Чому?\n",
    "\n",
    "Операцію MPI_ANY_SOURCE (як і MPI_ANY_TAG) не можна використовувати в операціях відправлення (Send), її можна використовувати лише в операціях прийому (Recv). Це пов'язано з тим, що при відправленні повідомлення відправник повинен чітко знати, кому саме він надсилає дані, тоді як одержувач може дозволити собі чекати повідомлення від будь-кого.\n",
    "\n",
    "9. Як визначити номер процесу-відправника, якщо під час прийому використовуються MPI_ANY_SOURCE, MPI_ANY_TAG?\n",
    "\n",
    "Якщо при прийомі використовувалися джокери (wildcards), то реальний номер процесу-відправника та тег повідомлення можна дізнатися, перевіривши поля MPI_SOURCE та MPI_TAG у структурі status, яка передається у функцію MPI_Recv і заповнюється нею після успішного отримання повідомлення.\n",
    "\n",
    "10. Чи означає повернення з функції MPI_Recv, що сталася помилка?\n",
    "\n",
    "Ні, саме по собі повернення з функції MPI_Recv означає лише, що операція прийому завершилася (повідомлення отримано і записано в буфер). Щоб дізнатися, чи сталася помилка, необхідно перевірити код повернення функції: якщо він дорівнює MPI_SUCCESS, то помилки не було.\n",
    "\n",
    "11. Чи потрібно перед викликом функції MPI_Recv звернутися до функції MPI_Get_Count?\n",
    "\n",
    "Ні, перед викликом MPI_Recv не потрібно звертатися до MPI_Get_Count. Функція MPI_Get_Count зазвичай викликається після успішного завершення MPI_Recv (використовуючи отриманий статус), щоб дізнатися фактичний розмір отриманих даних у одиницях вказаного типу даних.\n",
    "\n",
    "12. Чи можна використовувати функцію MPI_Recv, якщо не відомий відправник повідомлення чи тег повідомлення?\n",
    "\n",
    "Так, функцію MPI_Recv можна використовувати навіть за відсутності інформації про відправника або тег. Для цього замість конкретного рангу відправника вказують константу MPI_ANY_SOURCE, а замість конкретного тега — MPI_ANY_TAG.\n",
    "\n",
    "13. Як визначити довжину надісланого повідомлення?\n",
    "\n",
    "Довжину надісланого (точніше, отриманого) повідомлення можна визначити після прийому, викликавши функцію MPI_Get_Count. Їй передається структура status (заповнена функцією MPI_Recv) та тип даних, а вона повертає кількість елементів цього типу, що містилися в повідомленні.\n",
    "\n",
    "14. Чи завжди status.MPI_SOURCE=SOURCE, status.MPI_TAG=MPI_TAG?\n",
    "\n",
    "Ні, не завжди. Якщо у функції MPI_Recv в аргументах SOURCE та TAG були вказані конкретні значення, то поля статусу будуть їм дорівнювати. Однак, якщо використовувалися константи MPI_ANY_SOURCE або MPI_ANY_TAG, то в status.MPI_SOURCE та status.MPI_TAG будуть записані фактичні ідентифікатори відправника та тега отриманого повідомлення, які можуть відрізнятися від \"джокерів\".\n",
    "\n",
    "15. Що станеться під час виклику функції MPI_Buffer_attach, якщо недостатньо пам'яті для буфера?\n",
    "\n",
    "Якщо системі не вдається виділити або приєднати вказаний обсяг пам'яті під час виклику MPI_Buffer_attach (хоча зазвичай пам'ять виділяє користувач і передає вказівник), функція поверне відповідний код помилки, що сигналізує про проблему з ресурсами.\n",
    "\n",
    "16. Скільки буферів можна приєднати до процесу за один виклик MPI_Buffer_attach?\n",
    "\n",
    "За один виклик MPI_Buffer_attach можна приєднати лише один буфер. Більше того, до одного процесу в кожен момент часу може бути приєднаний тільки один активний буфер для буферизованого обміну; якщо потрібно змінити буфер, попередній треба від'єднати.\n",
    "\n",
    "17. У якому режимі мають бути надіслані повідомлення для використання буфера у MPI_Buffer_attach?\n",
    "\n",
    "Для того щоб використовувався буфер, приєднаний через MPI_Buffer_attach, повідомлення повинні відправлятися у буферизованому режимі комунікації. Для цього використовуються спеціальні функції відправки, такі як MPI_Bsend.\n",
    "\n",
    "18. Коли функція MPI_Buffer_detach може бути викликана?\n",
    "\n",
    "Функція MPI_Buffer_detach може бути викликана тоді, коли процес завершив відправлення всіх повідомлень у буферизованому режимі і хоче повернути собі контроль над пам'яттю буфера. Якщо в буфері ще залишаються невідправлені повідомлення, функція заблокується доти, доки вони не будуть фізично передані.\n",
    "\n",
    "19. Перерахуйте основні властивості парного обміну, які гарантує правильна реалізація MPI.\n",
    "\n",
    "Правильна реалізація MPI гарантує надійність доставки повідомлень, збереження порядку (non-overtaking) повідомлень між тією ж парою процесів з однаковими комунікатором і тегом, а також прогрес комунікацій (операції не повинні блокуватися вічно, якщо є відповідні пари Send/Recv).\n",
    "\n",
    "20. Чи гарантують властивості парного обміну, що програма з передачею повідомлень детермінована?\n",
    "\n",
    "Ні, властивості парного обміну не гарантують повної детермінованості програми. Якщо використовуються операції з MPI_ANY_SOURCE, порядок обробки повідомлень залежатиме від того, яке повідомлення фізично надійде першим, що може змінюватися від запуску до запуску через затримки мережі.\n",
    "\n",
    "21. У яких ситуаціях нестача буферного простору веде до дідлока?\n",
    "\n",
    "Нестача буферного простору веде до дідлока (взаємного блокування), коли два процеси намагаються одночасно відправити один одному повідомлення за допомогою блокуючих операцій (MPI_Send), розмір яких перевищує доступний системний буфер. Обидві функції Send заблокуються в очікуванні Recv, але Recv не може бути викликаний, оскільки процеси застрягли на Send.\n",
    "\n",
    "22. Сформулюйте основні правила відповідності типів даних.\n",
    "\n",
    "Тип даних, вказаний при відправленні, повинен відповідати типу даних, вказаному при прийомі. Це означає, що сигнатури типів мають збігатися (наприклад, якщо відправляється масив цілих чисел MPI_INT, то і прийматися має масив MPI_INT). Довжина повідомлення у байтах також узгоджується, але кількість елементів може відрізнятися, якщо розмір буфера одержувача достатній.\n",
    "\n",
    "23. Чи тягне обмін у MPI перетворення типів даних за її невідповідності?\n",
    "\n",
    "MPI виконує перетворення представлення даних (data representation conversion), наприклад, зміну порядку байтів (little-endian vs big-endian) при передачі між різнорідними архітектурами. Проте MPI не виконує автоматичне перетворення типів даних у розумінні мов програмування (наприклад, автоматичне перетворення float у int не відбувається; типи мають відповідати).\n",
    "\n",
    "25. Чи є MPI-програми, в яких змішані мови, які переносяться?\n",
    "\n",
    "Так, MPI спроектований таким чином, щоб забезпечувати сумісність між різними мовами програмування (наприклад, C, C++, Fortran). Оскільки типи даних MPI стандартизовані, програми, написані різними мовами, можуть коректно обмінюватися даними, що робить змішані програми переносимими.\n",
    "\n",
    "25. Що таке стандартний комунікаційний режим?\n",
    "\n",
    "Стандартний комунікаційний режим — це режим відправлення (функція MPI_Send), у якому бібліотека MPI самостійно вирішує, чи використовувати буферизацію вихідного повідомлення, чи блокувати процес до початку прийому. Це компромісний режим, який не гарантує ні негайної буферизації, ні синхронності.\n",
    "\n",
    "26. Чи буферизується вихідне повідомлення за стандартного комунікаційного режимі?\n",
    "\n",
    "Це залежить від реалізації MPI та розміру повідомлення. Якщо повідомлення мале і є вільна системна пам'ять, MPI зазвичай буферизує його. Якщо повідомлення велике, буферизація не відбувається, і відправлення блокується. Стандарт не дає жорстких гарантій щодо буферизації у цьому режимі.\n",
    "\n",
    "27. У разі використання функцій MPI_Send і MPI_Recv призведе до дідлоку?\n",
    "\n",
    "Використання MPI_Send і MPI_Recv призведе до дідлоку, якщо два процеси ініціюють обмін зустрічними повідомленнями (наприклад, обидва спочатку викликають Send до іншого, а потім Recv), і при цьому системного буфера недостатньо для збереження повідомлень. У такому разі обидва Send заблокуються назавжди.\n",
    "\n",
    "28. Перерахуйте три додаткові комунікаційні режими.\n",
    "\n",
    "Три додаткові режими комунікації (крім стандартного) — це: 1) Буферизований режим (Buffered, MPI_Bsend), 2) Синхронний режим (Synchronous, MPI_Ssend), 3) Режим готовності (Ready, MPI_Rsend).\n",
    "\n",
    "29. Скільки операцій прийому існує три різних режимів передачі?\n",
    "\n",
    "Існує лише одна універсальна операція прийому — MPI_Recv (або її неблокуючий аналог MPI_Irecv). Вона здатна приймати повідомлення, надіслані в будь-якому з чотирьох режимів передачі (стандартному, буферизованому, синхронному чи по готовності).\n",
    "\n",
    "30. Чому знадобилося введення різних комунікаційних режимів?\n",
    "\n",
    "Різні режими були введені для того, щоб дати програмісту можливість балансувати між продуктивністю, споживанням пам'яті та потребами в синхронізації. Наприклад, стандартний режим оптимізований для загальних випадків, синхронний — для детермінованої передачі, буферизований — для уникнення блокувань ціною пам'яті, а по готовності — для максимальної швидкості за умови гарантованої синхронізації.\n",
    "\n",
    "31. Чому посилка в стандартному режимі комунікації є локальною операцією?\n",
    "\n",
    "Посилка в стандартному режимі є локальною операцією лише у тому випадку, якщо повідомлення успішно буферизовано системою (наприклад, воно мале). Тоді функція повертається відразу після копіювання даних у системний буфер, не чекаючи дій від одержувача.\n",
    "\n",
    "32. Що станеться, якщо при буферизованій посилці недостатньо місця для повідомлення?\n",
    "\n",
    "Якщо при використанні буферизованої посилки (MPI_Bsend) розмір повідомлення перевищує вільне місце у підключеному користувачем буфері (або буфер взагалі не був підключений), виклик функції завершиться помилкою.\n",
    "\n",
    "33. У чому основна різниця між буферизованим та стандартним режимами?\n",
    "\n",
    "Основна різниця в тому, що буферизований режим (MPI_Bsend) гарантує використання наданого користувачем буфера і завжди завершується локально (не блокує відправника), тоді як стандартний режим (MPI_Send) покладається на системні буфери і може заблокувати відправника, якщо система вирішить не буферизувати повідомлення.\n",
    "\n",
    "34. Чи можна розглядати синхронний режим комунікацій як спосіб синхронізації процесів?\n",
    "\n",
    "Так, синхронний режим (MPI_Ssend) можна і часто використовують як засіб синхронізації (\"рандеву\"), оскільки успішне завершення функції відправлення на стороні відправника гарантує, що одержувач вже дійшов до відповідної точки у коді і почав прийом повідомлення.\n",
    "\n",
    "35. У чому основна різниця між синхронним та стандартним режимами?\n",
    "\n",
    "Різниця полягає в умові завершення: синхронна відправка (MPI_Ssend) не завершиться, поки одержувач не почне прийом, незалежно від розміру повідомлення. Стандартна відправка (MPI_Send) може завершитися раніше, якщо повідомлення було буферизовано, навіть якщо одержувач ще не готовий.\n",
    "\n",
    "36. У чому основна відмінність між стандартним та режимом готовності?\n",
    "\n",
    "Режим готовності (MPI_Rsend) вимагає, щоб операція прийому (Recv) була вже ініційована на приймаючій стороні до моменту виклику Rsend, інакше результат невизначений (можлива помилка). Стандартний режим не має такої вимоги і коректно обробляє ситуацію, коли Send викликається раніше за Recv.\n",
    "\n",
    "37. У чому різниця між блокуючим та неблокуючим обміном?\n",
    "\n",
    "Блокуючий обмін (Send/Recv) зупиняє виконання програми до повного завершення операції (передачі даних або їх копіювання), тоді як неблокуючий обмін (Isend/Irecv) лише ініціює операцію і миттєво повертає керування програмі, дозволяючи виконувати обчислення паралельно з передачею даних.\n",
    "\n",
    "38. Що таке приховані запити?\n",
    "\n",
    "Приховані запити (opaque request objects) — це системні об'єкти, доступ до яких здійснюється через дескриптори типу MPI_Request. Вони зберігають інформацію про стан запущеної неблокуючої операції комунікації, але їх внутрішня структура прихована від програміста.\n",
    "\n",
    "39. Чи вірний виклик функції MPI_IRecv(buf, 1, MPI_INT, 3, tag, comm, &status); для того, щоб прийняти одне ціле число від процесу 3?\n",
    "\n",
    "Ні, цей виклик невірний. Останнім аргументом функції MPI_Irecv має бути вказівник на змінну типу MPI_Request (дескриптор запиту), а не на MPI_Status. Статус перевіряється пізніше, при виклику функцій MPI_Wait або MPI_Test, які використовують цей запит.\n",
    "\n",
    "40. Визначте поняття “завершення операції посилки” для різних комунікаційних режимів.\n",
    "\n",
    "Завершення операції посилки означає, що буфер відправлення можна безпечно перезаписувати. Для буферизованого режиму це момент копіювання в буфер; для синхронного — момент початку прийому одержувачем; для стандартного — або копіювання в системний буфер, або завершення передачі.\n",
    "\n",
    "41. Визначте поняття “завершення операції прийому” для різних комунікаційних режимів.\n",
    "\n",
    "Завершення операції прийому для будь-якого режиму означає, що дані повністю надійшли з мережі або буфера, записані у пам'ять одержувача і готові до використання.\n",
    "\n",
    "42. Що таке активний та неактивний дескриптор?\n",
    "\n",
    "Активний дескриптор (request) пов'язаний з операцією, яка була ініційована, але ще не завершена (або не пройшла процедуру очищення через Wait/Test). Неактивний дескриптор має значення MPI_REQUEST_NULL; він означає, що жодна операція з ним не пов'язана (або попередня операція вже повністю завершена і дескриптор звільнено).\n",
    "\n",
    "43. У чому різниця використання MPI_Wait та MPI_Test?\n",
    "\n",
    "MPI_Wait блокує виконання процесу доти, доки операція, пов'язана з дескриптором, не завершиться. MPI_Test не блокує процес: він лише перевіряє поточний стан операції, повертає логічний прапорець (завершено/не завершено) і дозволяє програмі продовжувати роботу.\n",
    "\n",
    "44. Чи означає повернення з функції MPI_Wait, що всі процеси дійшли до бар'єра чи відправник повернувся з функції MPI_Send, чи одержувач повернувся з функції MPI_Recv, чи закінчилася асинхронно запущена операція?\n",
    "\n",
    "Повернення з MPI_Wait означає лише те, що закінчилася конкретна асинхронна операція (Isend або Irecv), пов'язана з переданим їй запитом. Це не є глобальним бар'єром і не дає інформації про стан інших процесів, окрім тієї, що випливає з семантики завершення конкретної передачі.\n",
    "\n",
    "45. Які властивості неблокуючого парного обміну повинні бути гарантовані правильною реалізацією MPI?\n",
    "\n",
    "Повинна бути гарантована можливість перекриття комунікацій та обчислень (overlap), збереження порядку повідомлень (як і в блокуючому обміні) та коректне звільнення ресурсів після виклику функцій завершення (Wait/Test).\n",
    "\n",
    "46. Закінчилася одна або всі з асинхронно запущених операцій, асоційованих із зазначеними у списку параметрів ідентифікаторами requests, якщо значення параметра flag дорівнює 1 при поверненні функції MPI_TestAll?\n",
    "\n",
    "Якщо flag дорівнює 1 при поверненні MPI_TestAll, це означає, що закінчилися абсолютно всі операції, асоційовані з переданим масивом запитів.\n",
    "\n",
    "47. У чому різниця використання функцій MPI_Waitany, MPI_Waitall, MPI_Waitsome?\n",
    "\n",
    "MPI_Waitany блокує процес до завершення однієї будь-якої операції зі списку. MPI_Waitall блокує до завершення всіх операцій зі списку. MPI_Waitsome блокує до завершення хоча б однієї операції і повертає список усіх, що завершилися на даний момент.\n",
    "\n",
    "48. У чому різниця використання функцій MPI_Waitany, MPI_Testany?\n",
    "\n",
    "MPI_Waitany блокує виконання програми доки не завершиться хоча б одна операція. MPI_Testany не блокує виконання: вона миттєво перевіряє, чи завершилася хоча б одна операція, і якщо ні, повертає прапорець false, дозволяючи продовжити роботу.\n",
    "\n",
    "49. Якщо кілька операцій із масиву активних запитів можуть завершитися одночасно під час виклику MPI_Waitany, який номер процесу буде обраний?\n",
    "\n",
    "Якщо кілька операцій завершилися одночасно, MPI_Waitany поверне індекс однієї з них. Стандарт не визначає, якої саме (це залежить від реалізації), тому не можна покладатися на певний порядок вибору (\"справедливість\" не гарантується).\n",
    "\n",
    "50. Що означає код MPI_ERR_Pending у статусі під час виконання функції MPI_Waitall?\n",
    "\n",
    "Код MPI_ERR_PENDING (або аналогічний статус у масиві статусів) у контексті функцій типу Waitsome/Testsome вказує на те, що дана конкретна операція ще не завершена, в той час як функція повернула керування через завершення інших операцій або помилки в інших запитах."
   ],
   "id": "c1049e610dacf289"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
